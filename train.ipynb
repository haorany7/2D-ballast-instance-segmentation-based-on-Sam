{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","mount_file_id":"1u1L-pfTAK9dz4FdyAsq2spCwk8Qcey20","authorship_tag":"ABX9TyPpwvZ9nAwLRaZaBOdFYhhA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["`all_data` is a list where each element is a dictionary that represents a mask. Each dictionary contains three keys:\n","\n","* `'segmentation'`: This key maps to a 2D numpy array representing the mask itself. Each element of this array corresponds to a pixel in the mask, with a value of 1 indicating that the pixel is part of the mask and a value of 0 indicating that it's not.\n","\n","* `'area'`: This key maps to an integer representing the total number of pixels in the mask (i.e., the number of 1s in the 'segmentation' array).\n","\n","* `'label'`: This key maps to a string indicating the type of the mask. It can take on two possible values: 'ballast' or 'non_ballast', depending on which type of object the mask represents. \n","\n","For example, if there are N masks in total (regardless of whether they are ballast or non-ballast), `all_data` would be a list of length N. Each element of the list would be a dictionary representing one mask. The mask's segmentation (in the form of a 2D array), area (as an integer), and label (as a string) are accessible via the 'segmentation', 'area', and 'label' keys, respectively."],"metadata":{"id":"t19uyCz9F1PT"}},{"cell_type":"code","source":["import os\n","import pickle"],"metadata":{"id":"DGL1wqfPMBkW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This part of the code loads `all_data` and transforms it into a dictionary with two fields `'segmentation'` and `'label'`. The label of each are generated automatically depending on the prefix of the folder from which the ballast file is loaded. For example, If the prefix of the folder is 'ballast', then the label of all masks contained in the files under the folder will be 'ballast' and vice verse."],"metadata":{"id":"njGgRs-sfZ2u"}},{"cell_type":"code","source":["def load_specific_files(ballast_files, non_ballast_files):\n","    all_data = []\n","    for files, label in [(ballast_files, 'ballast'), (non_ballast_files, 'non_ballast')]:\n","        for filepath in files:\n","            with open(filepath, 'rb') as f:\n","                masks = pickle.load(f)\n","                for mask in masks:\n","                    mask['label'] = label\n","                all_data.extend(masks)\n","    return all_data\n","\n","# Specify the paths to the specific files you want to process\n","ballast_files = [\n","    '/content/drive/MyDrive/Colab Notebooks/ballast_masks_training/surface_1.png_masks.pkl',\n","    # Add paths to other files as needed...\n","    '/content/drive/MyDrive/Colab Notebooks/ballast_masks_training/TTC_1.png_masks.pkl',   \n","    '/content/drive/MyDrive/Colab Notebooks/ballast_masks_training/TTC_4.png_masks.pkl',\n","    '/content/drive/MyDrive/Colab Notebooks/ballast_masks_training/TTC_5.png_masks.pkl'\n","]\n","non_ballast_files = [\n","    '/content/drive/MyDrive/Colab Notebooks/non_ballast_masks_training/image_10_05.jpg_masks.pkl',\n","    # Add paths to other files as needed...\n","    '/content/drive/MyDrive/Colab Notebooks/non_ballast_masks_training/image_11_25.jpg_masks.pkl',\n","    '/content/drive/MyDrive/Colab Notebooks/non_ballast_masks_training/image_22.jpg_masks.pkl',\n","    '/content/drive/MyDrive/Colab Notebooks/non_ballast_masks_training/image_25.jpg_masks.pkl',\n","    '/content/drive/MyDrive/Colab Notebooks/non_ballast_masks_training/image_30.jpg_masks.pkl',\n","]\n","\n","training_data = load_specific_files(ballast_files, non_ballast_files)\n"],"metadata":{"id":"s0OZt-yiG_mR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Firstly, we'll establish a function that calculates the bounding box encompassing non-zero pixels within a two-dimensional tensor, which corresponds to each individual mask. Subsequent to this, every box will be reformatted to a size of 64x64. Implementing these steps enables us to mitigate any potential influence arising from the area, which serves as a distorting factor.Simultaneously, given that a majority of the area in our mask is devoid of data, this type of positioning can help our model concentrate more effectively on regions with non-zero values."],"metadata":{"id":"v_4G_Hd_SSlt"}},{"cell_type":"code","source":["import numpy as np\n","def compute_bounding_box(mask):\n","    rows = np.any(mask, axis=1)\n","    cols = np.any(mask, axis=0)\n","    rmin, rmax = np.where(rows)[0][[0, -1]]\n","    cmin, cmax = np.where(cols)[0][[0, -1]]\n","\n","    return rmin, rmax, cmin, cmax\n"],"metadata":{"id":"mkQEeZo4SNql"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This function loops over your data, extracts the segmentation masks and labels, converts the masks to tensors, reshapes the tensors to add an extra dimension for the grayscale channel, and pairs each tensor with its corresponding label in a tuple."],"metadata":{"id":"i8YHuxlfNTaH"}},{"cell_type":"code","source":["import torch\n","import cv2  # for resizing images\n","\n","def to_tensor(data):\n","    tensor_data = []\n","    for item in data:\n","        mask = item['segmentation']\n","        label = 1 if item['label'] == 'ballast' else 0  # Convert labels to binary values\n","\n","        # Compute the bounding box around non-zero pixels\n","        rmin, rmax, cmin, cmax = compute_bounding_box(mask)\n","\n","        # Extract the ROI from the mask\n","        roi = mask[rmin:rmax+1, cmin:cmax+1]\n","\n","        # Resize the ROI to the standard input size for your network\n","        roi_resized = cv2.resize(roi.astype('float32'), (64, 64))\n","\n","        # Convert the numpy array to a PyTorch tensor\n","        roi_tensor = torch.Tensor(roi_resized)\n","\n","        # Add an extra dimension for the single channel (grayscale)\n","        roi_tensor = roi_tensor.unsqueeze(0)\n","\n","        tensor_data.append((roi_tensor, label))\n","\n","    return tensor_data\n","\n","\n"],"metadata":{"id":"29DC1pDnMJE2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once you've converted your data to tensors, you can use PyTorch's DataLoader class to handle batching and shuffling of the data:"],"metadata":{"id":"NtQjEg-PNVwm"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","tensor_data = to_tensor(training_data)\n","\n"],"metadata":{"id":"XPlroZUsMukL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fantastic! We are ready to begin training your convolutional neural network model on the processed and prepared dataset."],"metadata":{"id":"gC7kPHCwU8UW"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","# Model definition\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.bn1 = nn.BatchNorm2d(32)  # BatchNorm for first Conv layer\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        self.bn2 = nn.BatchNorm2d(64)  # BatchNorm for second Conv layer\n","        self.dropout1 = nn.Dropout2d(0.1)\n","        self.dropout2 = nn.Dropout2d(0.25)\n","        \n","        # Automatically calculate size\n","        x = torch.randn(64,64).view(-1,1,64,64)\n","        self._to_linear = None\n","        self.convs(x)\n","\n","        self.fc1 = nn.Linear(self._to_linear, 128)\n","        self.fc2 = nn.Linear(128, 2)\n","\n","    def convs(self, x):\n","        x = F.relu(self.bn1(self.conv1(x)))  # Apply BatchNorm after first Conv layer\n","        x = F.max_pool2d(x, (2, 2))\n","        x = F.relu(self.bn2(self.conv2(x)))  # Apply BatchNorm after second Conv layer\n","        x = F.max_pool2d(x, (2, 2))\n","        if self._to_linear is None:\n","            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n","        return x\n","\n","    def forward(self, x):\n","        x = self.convs(x)\n","        x = x.view(-1, self._to_linear)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout2(x)  # Use Dropout after the first fully connected layer\n","        x = self.fc2(x)\n","        return F.log_softmax(x, dim=1)\n","\n","\n","\n","\n","# Training loop\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = Net().to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n","\n","\n","# Split your data into training and validation sets\n","# Here, replace dataset with your actual dataset name\n","train_size = int(0.8 * len(tensor_data))\n","val_size = len(tensor_data) - train_size\n","train_dataset, val_dataset = torch.utils.data.random_split(tensor_data, [train_size, val_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n","\n","n_epochs = 200\n","patience = 40\n","min_val_loss = np.inf\n","counter = 0\n","\n","for epoch in range(n_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    train_loss = running_loss / len(train_loader)\n","\n","    model.eval()\n","    running_val_loss = 0.0\n","    with torch.no_grad():\n","        for i, data in enumerate(val_loader, 0):\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            running_val_loss += loss.item()\n","\n","    val_loss = running_val_loss / len(val_loader)\n","    print(f'Epoch {epoch + 1}, Train loss: {train_loss}, Val loss: {val_loss}')\n","\n","    # Early stopping\n","    if val_loss < min_val_loss:\n","        min_val_loss = val_loss\n","        counter = 0\n","    else:\n","        counter += 1\n","        print(f'EarlyStopping counter: {counter} out of {patience}')\n","        if counter >= patience:\n","            print('Early stopping')\n","            break\n","    scheduler.step()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4scTYvvvW2VF","executionInfo":{"status":"ok","timestamp":1686108830653,"user_tz":300,"elapsed":11089,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"f6c5d368-3918-418b-f220-599bd7a9eebf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Train loss: 2.6661027936966386, Val loss: 0.5607708856463433\n","Epoch 2, Train loss: 0.4499142158341098, Val loss: 0.46164959371089936\n","Epoch 3, Train loss: 0.4381609421658826, Val loss: 0.44669626504182813\n","Epoch 4, Train loss: 0.43311976980079303, Val loss: 0.5641560466960073\n","EarlyStopping counter: 1 out of 40\n","Epoch 5, Train loss: 0.42570113129430004, Val loss: 0.4618702784180641\n","EarlyStopping counter: 2 out of 40\n","Epoch 6, Train loss: 0.41554828201021465, Val loss: 0.7070583656430245\n","EarlyStopping counter: 3 out of 40\n","Epoch 7, Train loss: 0.4225700729853147, Val loss: 0.5356265634298325\n","EarlyStopping counter: 4 out of 40\n","Epoch 8, Train loss: 0.4228527994899007, Val loss: 0.44764395290985703\n","EarlyStopping counter: 5 out of 40\n","Epoch 9, Train loss: 0.4176855453036048, Val loss: 0.4795678235590458\n","EarlyStopping counter: 6 out of 40\n","Epoch 10, Train loss: 0.41109686412594537, Val loss: 0.7032594518037513\n","EarlyStopping counter: 7 out of 40\n","Epoch 11, Train loss: 0.40467613606483904, Val loss: 0.45792252868413924\n","EarlyStopping counter: 8 out of 40\n","Epoch 12, Train loss: 0.4131929179677716, Val loss: 0.4564038775861263\n","EarlyStopping counter: 9 out of 40\n","Epoch 13, Train loss: 0.4243354013600907, Val loss: 0.4649042084813118\n","EarlyStopping counter: 10 out of 40\n","Epoch 14, Train loss: 0.40890405201292657, Val loss: 0.45455442663860596\n","EarlyStopping counter: 11 out of 40\n","Epoch 15, Train loss: 0.4145235289613922, Val loss: 0.4835512638092041\n","EarlyStopping counter: 12 out of 40\n","Epoch 16, Train loss: 0.408458817314792, Val loss: 0.4590761050581932\n","EarlyStopping counter: 13 out of 40\n","Epoch 17, Train loss: 0.39787310129636294, Val loss: 0.4574775092303753\n","EarlyStopping counter: 14 out of 40\n","Epoch 18, Train loss: 0.4048701644717873, Val loss: 0.45783054642379284\n","EarlyStopping counter: 15 out of 40\n","Epoch 19, Train loss: 0.38591183818779984, Val loss: 0.4492653377354145\n","EarlyStopping counter: 16 out of 40\n","Epoch 20, Train loss: 0.39436755435807364, Val loss: 0.4826122522354126\n","EarlyStopping counter: 17 out of 40\n","Epoch 21, Train loss: 0.4163061330070743, Val loss: 0.5996081210672856\n","EarlyStopping counter: 18 out of 40\n","Epoch 22, Train loss: 0.44260511885989795, Val loss: 0.4527122475206852\n","EarlyStopping counter: 19 out of 40\n","Epoch 23, Train loss: 0.4563126008618962, Val loss: 0.5314580932259559\n","EarlyStopping counter: 20 out of 40\n","Epoch 24, Train loss: 0.417118763188263, Val loss: 0.45196334049105646\n","EarlyStopping counter: 21 out of 40\n","Epoch 25, Train loss: 0.39625276547747773, Val loss: 0.45130453556776046\n","EarlyStopping counter: 22 out of 40\n","Epoch 26, Train loss: 0.41584284448778475, Val loss: 0.5834490589797496\n","EarlyStopping counter: 23 out of 40\n","Epoch 27, Train loss: 0.45898596342507897, Val loss: 0.47274216413497927\n","EarlyStopping counter: 24 out of 40\n","Epoch 28, Train loss: 0.4074904390356757, Val loss: 0.4556989587843418\n","EarlyStopping counter: 25 out of 40\n","Epoch 29, Train loss: 0.4174193215447587, Val loss: 0.5069502257741988\n","EarlyStopping counter: 26 out of 40\n","Epoch 30, Train loss: 0.3984074190065458, Val loss: 0.47851376309990884\n","EarlyStopping counter: 27 out of 40\n","Epoch 31, Train loss: 0.4610260759855246, Val loss: 0.4472047109156847\n","EarlyStopping counter: 28 out of 40\n","Epoch 32, Train loss: 0.4061696268134303, Val loss: 0.5287024311721324\n","EarlyStopping counter: 29 out of 40\n","Epoch 33, Train loss: 0.38247228036453196, Val loss: 0.5083020836114883\n","EarlyStopping counter: 30 out of 40\n","Epoch 34, Train loss: 0.38382001646927427, Val loss: 0.4510985990986228\n","EarlyStopping counter: 31 out of 40\n","Epoch 35, Train loss: 0.3777662382110373, Val loss: 0.4952790187206119\n","EarlyStopping counter: 32 out of 40\n","Epoch 36, Train loss: 0.3745364615475977, Val loss: 0.49067545831203463\n","EarlyStopping counter: 33 out of 40\n","Epoch 37, Train loss: 0.40782485031462334, Val loss: 0.5134421430528164\n","EarlyStopping counter: 34 out of 40\n","Epoch 38, Train loss: 0.4145082442404388, Val loss: 0.5054290607571602\n","EarlyStopping counter: 35 out of 40\n","Epoch 39, Train loss: 0.37738578563386743, Val loss: 0.46929989457130433\n","EarlyStopping counter: 36 out of 40\n","Epoch 40, Train loss: 0.37195664566832704, Val loss: 0.4860554806888103\n","EarlyStopping counter: 37 out of 40\n","Epoch 41, Train loss: 0.3817322842486493, Val loss: 0.5744287848472596\n","EarlyStopping counter: 38 out of 40\n","Epoch 42, Train loss: 0.38814889610587777, Val loss: 0.5063034996390343\n","EarlyStopping counter: 39 out of 40\n","Epoch 43, Train loss: 0.36362419364514287, Val loss: 0.5304838918149472\n","EarlyStopping counter: 40 out of 40\n","Early stopping\n"]}]},{"cell_type":"markdown","source":["Now we turn to assessing the performance of our model."],"metadata":{"id":"Zi-f6r5_jBg_"}},{"cell_type":"code","source":["# Specify the paths to the specific test files you want to process\n","test_ballast_files = [\n","    '/content/drive/MyDrive/Colab Notebooks/ballast_masks_training/TTC_2.png_masks.pkl', \n","    # Add paths to other test ballast files as needed...\n","]\n","test_non_ballast_files = [\n","    '/content/drive/MyDrive/Colab Notebooks/non_ballast_masks_training/image_13_14.jpg_masks.pkl',\n","    # Add paths to other test non ballast files as needed...\n","]\n","\n","# Load the test data\n","test_data = load_specific_files(test_ballast_files, test_non_ballast_files)\n","\n","# Convert the test data to tensor format\n","test_tensor_data = to_tensor(test_data)\n"],"metadata":{"id":"TpEoUhn8LPB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset\n"],"metadata":{"id":"BlcRhy3EMw_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a DataLoader for your test data\n","test_dataset = TensorDataset(torch.stack([sample[0] for sample in test_tensor_data]),\n","                             torch.Tensor([sample[1] for sample in test_tensor_data]))\n","\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"jBcJsMZELxaK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()  # Set model to evaluation mode\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for data in test_loader:\n","        images, labels = data\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Test Accuracy of the model on the test images: {100 * correct / total}%')\n"],"metadata":{"id":"8VlhH96mM8g5","executionInfo":{"status":"ok","timestamp":1686108837469,"user_tz":300,"elapsed":122,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"1d4358ad-71a9-4afe-f6da-809cadbdf655","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy of the model on the test images: 81.52173913043478%\n"]}]},{"cell_type":"markdown","source":["Now save our model as \"model_001\""],"metadata":{"id":"HrvVg65lCBqm"}},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/model_trained/model_001.pt')\n"],"metadata":{"id":"eqmokHtz8GxK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Try whether we can load our model successfully"],"metadata":{"id":"SbuxjLLZCKAH"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","# Model definition\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.bn1 = nn.BatchNorm2d(32)  # BatchNorm for first Conv layer\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        self.bn2 = nn.BatchNorm2d(64)  # BatchNorm for second Conv layer\n","        self.dropout1 = nn.Dropout2d(0.1)\n","        self.dropout2 = nn.Dropout2d(0.25)\n","        \n","        # Automatically calculate size\n","        x = torch.randn(64,64).view(-1,1,64,64)\n","        self._to_linear = None\n","        self.convs(x)\n","\n","        self.fc1 = nn.Linear(self._to_linear, 128)\n","        self.fc2 = nn.Linear(128, 2)\n","\n","    def convs(self, x):\n","        x = F.relu(self.bn1(self.conv1(x)))  # Apply BatchNorm after first Conv layer\n","        x = F.max_pool2d(x, (2, 2))\n","        x = F.relu(self.bn2(self.conv2(x)))  # Apply BatchNorm after second Conv layer\n","        x = F.max_pool2d(x, (2, 2))\n","        if self._to_linear is None:\n","            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n","        return x\n","\n","    def forward(self, x):\n","        x = self.convs(x)\n","        x = x.view(-1, self._to_linear)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout2(x)  # Use Dropout after the first fully connected layer\n","        x = self.fc2(x)\n","        return F.log_softmax(x, dim=1)"],"metadata":{"id":"_OzO9CnRXcKl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With the classifier now trained and stored, we are prepared to reload it for the purpose of predicting labels."],"metadata":{"id":"pkEXASbWj5k-"}},{"cell_type":"code","source":["# You need to first create the model object\n","model = Net()\n","model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/model_trained/model_001.pt'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mnfTv1Wy_5_x","executionInfo":{"status":"ok","timestamp":1686236243937,"user_tz":300,"elapsed":6589,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"71565981-bbbc-4fd5-8856-1d5e1c1b4d09"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["def predict(segmentation, model):\n","    # Compute the bounding box around non-zero pixels\n","    rmin, rmax, cmin, cmax = compute_bounding_box(segmentation)\n","\n","    # Extract the ROI from the mask\n","    roi = segmentation[rmin:rmax+1, cmin:cmax+1]\n","\n","    # Resize the ROI to the standard input size for your network\n","    roi_resized = cv2.resize(roi.astype('float32'), (64, 64))\n","\n","    # Convert the numpy array to a PyTorch tensor\n","    roi_tensor = torch.Tensor(roi_resized)\n","\n","    # Add an extra dimension for the single channel (grayscale)\n","    roi_tensor = roi_tensor.unsqueeze(0)\n","    \n","    # Adding a batch dimension\n","    roi_tensor = roi_tensor.unsqueeze(0)\n","\n","    # Transfer to device\n","    roi_tensor = roi_tensor.to(device)\n","\n","    # Move the model to the same device\n","    model = model.to(device)\n","\n","    # Make prediction\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(roi_tensor)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","    # Return True if the model predicts the mask to be a ballast, False otherwise\n","    return predicted.item() == 1\n","\n","\n","\n","def filter_masks(masks, model):\n","    output_masks = []\n","    for mask in masks:\n","        if predict(mask['segmentation'], model):\n","            output_masks.append(mask)\n","    return output_masks\n","\n"],"metadata":{"id":"lfajdLlT_3zf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This function, provided by Meta, is utilized to overlay the masks on the segmented image for visualization."],"metadata":{"id":"pHzmRASwj_cl"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","import cv2\n","def show_anns(anns):\n","    if len(anns) == 0:\n","        return\n","    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n","    ax = plt.gca()\n","    ax.set_autoscale_on(False)\n","\n","    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n","    img[:,:,3] = 0\n","    for ann in sorted_anns:\n","        m = ann['segmentation']\n","        color_mask = np.concatenate([np.random.random(3), [0.35]])\n","        img[m] = color_mask\n","    ax.imshow(img)"],"metadata":{"id":"b3vhu6nhAExq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here comes the firsr test case, we want to chenk whether it can successfully filter out the non-ballasts."],"metadata":{"id":"Q2E7PnJjkX5J"}},{"cell_type":"code","source":["image = cv2.imread('/content/drive/MyDrive/Colab Notebooks/samples/12.png')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"],"metadata":{"id":"wm4fwGapAkka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(20,20))\n","plt.imshow(image)\n","plt.axis('off')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":878,"output_embedded_package_id":"1EUHDKOtA88F28IAXtMaMkWiqJUpbCThS"},"id":"BpMGtouUA6lB","executionInfo":{"status":"ok","timestamp":1686236286390,"user_tz":300,"elapsed":22148,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"cbcaba2e-315a-4cf7-daba-e3af10d57a48"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Some commands necessary to install dependencys as well as Sam model."],"metadata":{"id":"7lxLipeCkoFO"}},{"cell_type":"code","source":["from IPython.display import display, HTML\n","display(HTML(\n","\"\"\"\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\"\"\"\n","))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":41},"id":"gdGG4RasBP55","executionInfo":{"status":"ok","timestamp":1686236288764,"user_tz":300,"elapsed":176,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"272a4197-c17e-432c-8863-562f6f6176cc"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n"]},"metadata":{}}]},{"cell_type":"code","source":["using_colab = True"],"metadata":{"id":"3ROxtTqXBUV6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if using_colab:\n","    import torch\n","    import torchvision\n","    print(\"PyTorch version:\", torch.__version__)\n","    print(\"Torchvision version:\", torchvision.__version__)\n","    print(\"CUDA is available:\", torch.cuda.is_available())\n","    import sys\n","    !{sys.executable} -m pip install opencv-python matplotlib\n","    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n","    \n","    !mkdir images\n","    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n","        \n","    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tc23kJsOBXMq","executionInfo":{"status":"ok","timestamp":1686236321280,"user_tz":300,"elapsed":23242,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"fac6f452-87ff-4bfe-bfff-c6b6f83e4b84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.0.1+cu118\n","Torchvision version: 0.15.2+cu118\n","CUDA is available: True\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/facebookresearch/segment-anything.git\n","  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-k9yi0ba9\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-k9yi0ba9\n","  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: segment-anything\n","  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36589 sha256=37a476162b9fe54bb407cb08292ea7e577a16b2d73641c92c00c71d95c133bb6\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-c9w35tlp/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n","Successfully built segment-anything\n","Installing collected packages: segment-anything\n","Successfully installed segment-anything-1.0\n","--2023-06-08 14:58:31--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 99846 (98K) [image/jpeg]\n","Saving to: ‘images/dog.jpg’\n","\n","dog.jpg             100%[===================>]  97.51K  --.-KB/s    in 0.005s  \n","\n","2023-06-08 14:58:32 (19.8 MB/s) - ‘images/dog.jpg’ saved [99846/99846]\n","\n","--2023-06-08 14:58:32--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.10, 13.227.219.59, 13.227.219.70, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.10|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2564550879 (2.4G) [binary/octet-stream]\n","Saving to: ‘sam_vit_h_4b8939.pth’\n","\n","sam_vit_h_4b8939.pt 100%[===================>]   2.39G   255MB/s    in 11s     \n","\n","2023-06-08 14:58:43 (220 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n","\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","# sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)"],"metadata":{"id":"1Beb3vmCA-qZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Meta provides an very concise API to segment everything in the image automatically."],"metadata":{"id":"ng_zN3aVk3-N"}},{"cell_type":"code","source":["masks_demo = mask_generator.generate(image)"],"metadata":{"id":"I_to2FM6Bnc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(masks_demo))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nm-770W6mcOc","executionInfo":{"status":"ok","timestamp":1686238068662,"user_tz":300,"elapsed":173,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"e952f44f-7635-4d67-8bf4-091c96084348"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["528\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686,"output_embedded_package_id":"1W6YYPdm5J1VgLIr4st96jHereqwD-w-u"},"id":"77ac29c5","outputId":"3c4acbba-4cc3-49e1-e53d-0f6b03171ab7","scrolled":false,"executionInfo":{"status":"ok","timestamp":1686110835436,"user_tz":300,"elapsed":13601,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["plt.figure(figsize=(20,20))\n","plt.imshow(image)\n","show_anns(masks)\n","plt.axis('off')\n","plt.show() "]},{"cell_type":"markdown","source":["Now see what will happen after filtering the non-ballast objects."],"metadata":{"id":"Ipgr8OniCdJD"}},{"cell_type":"code","source":["filtered_masks_demo = filter_masks(masks_demo, model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Mx20QsRCbVx","executionInfo":{"status":"ok","timestamp":1686238117923,"user_tz":300,"elapsed":1254,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"8eb9334f-be5e-4bdb-f874-b53466924191"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]}]},{"cell_type":"code","source":["print(len(filtered_masks_demo))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x6rpoI56n86E","executionInfo":{"status":"ok","timestamp":1686238158496,"user_tz":300,"elapsed":175,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"b95e196c-928b-4036-b9e6-b7cad7496d3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["527\n"]}]},{"cell_type":"markdown","source":["\n","As anticipated, the classifier successfully eliminated the twigs located at the top-right and bottom-left corners."],"metadata":{"id":"yqCr9eAqlL0s"}},{"cell_type":"code","source":["plt.figure(figsize=(20,20))\n","plt.imshow(image)\n","show_anns(filtered_masks)\n","plt.axis('off')\n","plt.show() "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686,"output_embedded_package_id":"1TyvLjZyPUoea_sU3CKyIiRDcRd3XARiy"},"id":"gaZjnmReDUbH","executionInfo":{"status":"ok","timestamp":1686111102017,"user_tz":300,"elapsed":12701,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"680329ef-e8ee-4987-d59c-bf97c8afef67"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Another test on labeled image, which is annotated by myself."],"metadata":{"id":"yFBGLFMOXom2"}},{"cell_type":"code","source":["image2 = cv2.imread('/content/drive/MyDrive/Colab Notebooks/ballasts_img/surface_1.png')\n","image2 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"],"metadata":{"id":"GN31C-n5XneF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(20,20))\n","plt.imshow(image)\n","plt.axis('off')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":878,"output_embedded_package_id":"1XaBLnBXa9eFnTGt9__f9uwcJHvJ5ZRou"},"id":"V8jCT6e_YcK2","executionInfo":{"status":"ok","timestamp":1686238214625,"user_tz":300,"elapsed":22803,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"6f4bcd19-390f-48be-ce86-55de934924cc"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import sys\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","# sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)"],"metadata":{"id":"qOEdD3P5ZH8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["masks2 = mask_generator.generate(image2)"],"metadata":{"id":"hEWt3BHvZAR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(masks2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pttUGdVQpxV5","executionInfo":{"status":"ok","timestamp":1686238626353,"user_tz":300,"elapsed":178,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"2951ce2a-62eb-4ab4-d277-8d5ba401040f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["519\n"]}]},{"cell_type":"code","source":["plt.figure(figsize=(20,20))\n","plt.imshow(image)\n","show_anns(masks2)\n","plt.axis('off')\n","plt.show() "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":878,"output_embedded_package_id":"1hYZQtvOvP2gp2jRUZxEzXgfYmwzgfK7_"},"id":"8xWGSnswZWJO","executionInfo":{"status":"ok","timestamp":1686237304893,"user_tz":300,"elapsed":29382,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"aca226e7-6892-45d5-d02a-33e5e22927ca"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["filtered_masks2 = filter_masks(masks2, model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TmJshbsCZZRx","executionInfo":{"status":"ok","timestamp":1686238677837,"user_tz":300,"elapsed":1358,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"bc78c6af-0a3f-4b6d-9ac5-4f95e10c77d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]}]},{"cell_type":"code","source":["print(len(filtered_masks2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ts4aZq6jlL2D","executionInfo":{"status":"ok","timestamp":1686238679975,"user_tz":300,"elapsed":184,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"a315ac80-bec3-47d5-b9fe-2a2bc45859b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["516\n"]}]},{"cell_type":"markdown","source":["From our observations, even in the absence of twigs in the images, the downstream filter manages to retain the majority of the ballast, with only three instances erroneously removed. Therefore, we can infer that the classifier is functioning effectively."],"metadata":{"id":"UqcSBZDalsIC"}},{"cell_type":"code","source":["plt.figure(figsize=(20,20))\n","plt.imshow(image)\n","show_anns(filtered_masks)\n","plt.axis('off')\n","plt.show() "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686,"output_embedded_package_id":"1T2G-jPVJ08C_6dissLhjH371QIlI1v5T"},"id":"fPUHlOqlZht5","executionInfo":{"status":"ok","timestamp":1686184260275,"user_tz":300,"elapsed":47165,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"edd7fd02-6575-463d-87a6-512b5f0d003f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Now we need implement metrices to help determine the accuracy of out approach compared to the groud truth."],"metadata":{"id":"4rgG8VPzZjyf"}},{"cell_type":"markdown","source":["Step 1: Load the JSON data.\n","You need to open the JSON file and load the data from it:"],"metadata":{"id":"1P7tnYreag5O"}},{"cell_type":"code","source":["import json\n","\n","# Load the reference data\n","with open('/content/drive/MyDrive/Colab Notebooks/ballasts_img/surface_1_png.rf.e373bd37e96d074824edb4c4d9296655.json', 'r') as f:\n","    reference_data = json.load(f)\n"],"metadata":{"id":"zqwCgbMcZx-a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 2: Convert the reference data to polygons.\n","Here, we'll iterate through the reference data and convert each region to a polygon. Let's use the Polygon class from the shapely.geometry module:"],"metadata":{"id":"dI0E92jbacqX"}},{"cell_type":"code","source":["from shapely.geometry import Polygon\n","\n","# Convert the reference data to polygons\n","reference_polygons = []\n","for entry in reference_data.values():\n","    for region in entry['regions']:\n","        shape_attributes = region['shape_attributes']\n","        x_points = shape_attributes['all_points_x']\n","        y_points = shape_attributes['all_points_y']\n","        polygon = Polygon(zip(x_points, y_points))\n","        reference_polygons.append(polygon)\n"],"metadata":{"id":"kLQCQxMwaUnL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 3: Find the exterior of the predicted mask and convert it to a polygon. Here, we'll use the findContours function from the OpenCV library to find the exterior of the mask:"],"metadata":{"id":"gzts6556a50A"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","\n","\n","predicted_polygons = []\n","\n","for mask in masks:\n","    # Convert mask['segmentation'] to a proper binary image\n","    binary_mask = np.array(mask['segmentation'], dtype=np.uint8)\n","\n","    # Find the contours of the mask\n","    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    # Convert the contours to polygons\n","    for contour in contours:\n","        if len(contour) > 1:\n","            polygon = Polygon(np.squeeze(contour))\n","            predicted_polygons.append(polygon)\n","\n","\n"],"metadata":{"id":"ANwg6zp3a7GJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 4: Calculate the IoU for the predicted polygons and the reference polygons:"],"metadata":{"id":"NJKpuEvscTu1"}},{"cell_type":"code","source":["# Initialize counters for true positives, false positives, and false negatives\n","tp, fp, fn = 0, 0, 0\n","\n","# Threshold for IoU\n","iou_threshold = 0.5\n","\n","for predicted_polygon in predicted_polygons:\n","    # Check if the predicted polygon matches any of the reference polygons\n","    if any(predicted_polygon.intersection(reference_polygon).area / predicted_polygon.union(reference_polygon).area > iou_threshold for reference_polygon in reference_polygons):\n","        tp += 1\n","    else:\n","        fp += 1\n","\n","# Count the reference polygons that don't match any predicted polygons\n","for reference_polygon in reference_polygons:\n","    if not any(predicted_polygon.intersection(reference_polygon).area / predicted_polygon.union(reference_polygon).area > iou_threshold for predicted_polygon in predicted_polygons):\n","        fn += 1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swCHhsgZcQ_G","executionInfo":{"status":"ok","timestamp":1686184742284,"user_tz":300,"elapsed":12531,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"a3da3040-10ce-4f7a-ab19-3cd7caddfdc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n","  return lib.intersection(a, b, **kwargs)\n"]}]},{"cell_type":"markdown","source":["Step 5: Calculate precision, recall, and F1-score:"],"metadata":{"id":"dxsG0cqZcaOf"}},{"cell_type":"code","source":["# Calculate precision, recall, and F1-score\n","precision = tp / (tp + fp) if tp + fp > 0 else 0\n","recall = tp / (tp + fn) if tp + fn > 0 else 0\n","f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n","\n","print(f'Precision: {precision}, Recall: {recall}, F1-score: {f1_score}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LnH8liqHcXoV","executionInfo":{"status":"ok","timestamp":1686184776519,"user_tz":300,"elapsed":318,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"60e332f6-0fba-47fa-e648-71d7608d93f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 0.8336314847942755, Recall: 0.8910133843212237, F1-score: 0.8613678373382625\n"]}]},{"cell_type":"code","source":["filtered_polygons = []\n","\n","for mask in filtered_masks:\n","    # Convert mask['segmentation'] to a proper binary image\n","    binary_mask = np.array(mask['segmentation'], dtype=np.uint8)\n","\n","    # Find the contours of the mask\n","    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    # Convert the contours to polygons\n","    for contour in contours:\n","        if len(contour) > 1:\n","            polygon = Polygon(np.squeeze(contour))\n","            filtered_polygons.append(polygon)\n"],"metadata":{"id":"SBthD_RNc8_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize counters for true positives, false positives, and false negatives\n","tp, fp, fn = 0, 0, 0\n","\n","# Threshold for IoU\n","iou_threshold = 0.5\n","\n","# For each filtered (predicted) polygon...\n","for filtered_polygon in filtered_polygons:\n","    # Check if the filtered polygon matches any of the reference polygons\n","    if any(filtered_polygon.intersection(reference_polygon).area / filtered_polygon.union(reference_polygon).area > iou_threshold for reference_polygon in reference_polygons):\n","        tp += 1\n","    else:\n","        fp += 1\n","\n","# Count the reference polygons that don't match any filtered polygons\n","for reference_polygon in reference_polygons:\n","    if not any(filtered_polygon.intersection(reference_polygon).area / filtered_polygon.union(reference_polygon).area > iou_threshold for filtered_polygon in filtered_polygons):\n","        fn += 1\n","\n","# Calculate precision, recall, and F1-score\n","precision = tp / (tp + fp) if tp + fp > 0 else 0\n","recall = tp / (tp + fn) if tp + fn > 0 else 0\n","f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n","\n","print(f'Precision: {precision}, Recall: {recall}, F1-score: {f1_score}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UlJ2U-mZc-zs","executionInfo":{"status":"ok","timestamp":1686185188230,"user_tz":300,"elapsed":12334,"user":{"displayName":"Haoran Yuan","userId":"15740772057815789341"}},"outputId":"f5e636df-87bc-405c-8e4c-3efd84487489"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 0.8333333333333334, Recall: 0.8891013384321224, F1-score: 0.8603145235892692\n"]}]}]}